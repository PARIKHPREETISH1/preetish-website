[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "My Resume!\nDownload PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nPreetish Parikh\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nPreetish Parikh\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nPreetish Parikh\n\n\nMay 7, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project 3/index.html",
    "href": "blog/project 3/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\n\nlibrary(haven)\ndata &lt;- read_dta(\"C:/Users/parik/mysite/blog/project 3/karlan_list_2007.dta\")\n\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project 3/index.html#introduction",
    "href": "blog/project 3/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\n\nlibrary(haven)\ndata &lt;- read_dta(\"C:/Users/parik/mysite/blog/project 3/karlan_list_2007.dta\")\n\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project 3/index.html#data",
    "href": "blog/project 3/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\n+Description\n\n\n\n\n\n\nlibrary(haven)    # for reading .dta files\nlibrary(dplyr)    # for data manipulation\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load the dataset (adjust the path as necessary)\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# View structure\nstr(data)\n\ntibble [50,083 × 51] (S3: tbl_df/tbl/data.frame)\n $ treatment         : num [1:50083] 0 0 1 1 1 0 1 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"Treatment\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ control           : num [1:50083] 1 1 0 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"Control\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ ratio             : dbl+lbl [1:50083] 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 1, 3, 3...\n   ..@ label       : chr \"Match ratio\"\n   ..@ format.stata: chr \"%9.0g\"\n   ..@ labels      : Named num 0\n   .. ..- attr(*, \"names\")= chr \"Control\"\n $ ratio2            : num [1:50083] 0 0 0 0 0 0 0 1 1 0 ...\n  ..- attr(*, \"label\")= chr \"2:1 match ratio\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ ratio3            : num [1:50083] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"3:1 match ratio\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ size              : dbl+lbl [1:50083] 0, 0, 3, 4, 2, 0, 1, 3, 4, 1, 4, 2, 0, 1, 0, 4, 1, 4...\n   ..@ label       : chr \"Match threshold\"\n   ..@ format.stata: chr \"%9.0g\"\n   ..@ labels      : Named num [1:5] 0 1 2 3 4\n   .. ..- attr(*, \"names\")= chr [1:5] \"Control\" \"$25,000\" \"$50,000\" \"$100,000\" ...\n $ size25            : num [1:50083] 0 0 0 0 0 0 1 0 0 1 ...\n  ..- attr(*, \"label\")= chr \"$25,000 match threshold\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ size50            : num [1:50083] 0 0 0 0 1 0 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"$50,000 match threshold\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ size100           : num [1:50083] 0 0 1 0 0 0 0 1 0 0 ...\n  ..- attr(*, \"label\")= chr \"$100,000 match threshold\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ sizeno            : num [1:50083] 0 0 0 1 0 0 0 0 1 0 ...\n  ..- attr(*, \"label\")= chr \"Unstated match threshold\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ ask               : dbl+lbl [1:50083] 0, 0, 1, 1, 1, 0, 3, 3, 2, 2, 1, 3, 0, 2, 0, 1, 2, 3...\n   ..@ label       : chr \"Suggested donation amount\"\n   ..@ format.stata: chr \"%9.0g\"\n   ..@ labels      : Named num [1:4] 0 1 2 3\n   .. ..- attr(*, \"names\")= chr [1:4] \"Control\" \"1x\" \"1.25x\" \"1.50x\"\n $ askd1             : num [1:50083] 0 0 1 1 1 0 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"Suggested donation was highest previous contribution\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ askd2             : num [1:50083] 0 0 0 0 0 0 0 0 1 1 ...\n  ..- attr(*, \"label\")= chr \"Suggested donation was 1.25 x highest previous contribution\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ askd3             : num [1:50083] 0 0 0 0 0 0 1 1 0 0 ...\n  ..- attr(*, \"label\")= chr \"Suggested donation was 1.50 x highest previous contribution\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ ask1              : num [1:50083] 55 25 55 55 35 95 125 75 250 150 ...\n  ..- attr(*, \"label\")= chr \"Highest previous contribution (for suggestion)\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0gc\"\n $ ask2              : num [1:50083] 70 35 70 70 45 120 160 95 315 190 ...\n  ..- attr(*, \"label\")= chr \"1.25 x highest previous contribution (for suggestion)\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0gc\"\n $ ask3              : num [1:50083] 85 50 85 85 55 145 190 120 375 225 ...\n  ..- attr(*, \"label\")= chr \"1.50 x highest previous contribution (for suggestion)\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0gc\"\n $ amount            : num [1:50083] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"Dollars given\"\n  ..- attr(*, \"format.stata\")= chr \"%9.2f\"\n $ gave              : num [1:50083] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"Gave anything\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ amountchange      : num [1:50083] -45 -25 -50 -25 -15 -45 -50 -65 -100 -125 ...\n  ..- attr(*, \"label\")= chr \"Change in amount given\"\n  ..- attr(*, \"format.stata\")= chr \"%9.2fc\"\n $ hpa               : num [1:50083] 45 25 50 50 25 90 100 65 200 125 ...\n  ..- attr(*, \"label\")= chr \"Highest previous contribution\"\n  ..- attr(*, \"format.stata\")= chr \"%9.2fc\"\n $ ltmedmra          : num [1:50083] 0 1 0 1 1 0 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"Small prior donor: last gift was less than median $35\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ freq              : num [1:50083] 2 2 3 15 42 20 12 13 28 4 ...\n  ..- attr(*, \"label\")= chr \"Number of prior donations\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ years             : num [1:50083] 4 3 2 8 95 10 8 16 19 7 ...\n  ..- attr(*, \"label\")= chr \"Number of years since initial donation\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ year5             : num [1:50083] 0 0 0 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"At least 5 years since initial donation\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ mrm2              : num [1:50083] 31 5 6 1 24 3 4 4 6 35 ...\n  ..- attr(*, \"label\")= chr \"Number of months since last donation\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ dormant           : num [1:50083] 1 0 0 0 1 0 0 0 0 1 ...\n  ..- attr(*, \"label\")= chr \"Already donated in 2005\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ female            : num [1:50083] 0 0 0 0 1 0 1 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"Female\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ couple            : num [1:50083] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"Couple\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ state50one        : num [1:50083] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"State tag: 1 for one observation of each of 50 states; 0 otherwise\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ nonlit            : num [1:50083] 5 0 3 1 1 0 0 4 1 4 ...\n  ..- attr(*, \"label\")= chr \"Nonlitigation\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ cases             : num [1:50083] 4 2 1 2 1 0 1 3 1 3 ...\n  ..- attr(*, \"label\")= chr \"Court cases from state in 2004-5 in which organization was involved\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ statecnt          : num [1:50083] 4.5 2.98 9.61 3.28 2.3 ...\n  ..- attr(*, \"label\")= chr \"Percent of sample from state\"\n  ..- attr(*, \"format.stata\")= chr \"%9.2f\"\n $ stateresponse     : num [1:50083] 0.0199 0.0261 0.023 0.0207 0.0156 ...\n  ..- attr(*, \"label\")= chr \"Proportion of sample from the state who gave\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ stateresponset    : num [1:50083] 0.0195 0.0278 0.0222 0.0247 0.017 ...\n  ..- attr(*, \"label\")= chr \"Proportion of treated sample from the state who gave\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ stateresponsec    : num [1:50083] 0.0208 0.0225 0.0247 0.0127 0.0129 ...\n  ..- attr(*, \"label\")= chr \"Proportion of control sample from the state who gave\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ stateresponsetminc: num [1:50083] -0.0013 0.00534 -0.00258 0.01202 0.00408 ...\n  ..- attr(*, \"label\")= chr \"stateresponset - stateresponsec\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ perbush           : num [1:50083] 0.49 0.465 0.408 0.465 0.525 ...\n  ..- attr(*, \"label\")= chr \"State vote share for Bush\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ close25           : num [1:50083] 1 0 0 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"label\")= chr \"State vote share for Bush between 47.5% and 52.5%\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ red0              : num [1:50083] 0 0 0 0 1 1 1 0 1 0 ...\n  ..- attr(*, \"label\")= chr \"Red state\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ blue0             : num [1:50083] 1 1 1 1 0 0 0 1 0 1 ...\n  ..- attr(*, \"label\")= chr \"Blue state\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ redcty            : num [1:50083] 0 1 0 1 0 1 1 0 1 0 ...\n  ..- attr(*, \"label\")= chr \"Red county\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ bluecty           : num [1:50083] 1 0 1 0 1 0 0 1 0 1 ...\n  ..- attr(*, \"label\")= chr \"Blue county\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ pwhite            : num [1:50083] 0.446 NA 0.936 0.888 0.759 ...\n  ..- attr(*, \"label\")= chr \"Proportion white within zip code\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ pblack            : num [1:50083] 0.5278 NA 0.0119 0.0108 0.1274 ...\n  ..- attr(*, \"label\")= chr \"Proportion black within zip code\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ page18_39         : num [1:50083] 0.318 NA 0.276 0.279 0.442 ...\n  ..- attr(*, \"label\")= chr \"Proportion age 18-39 within zip code\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ ave_hh_sz         : num [1:50083] 2.1 NA 2.48 2.65 1.85 ...\n  ..- attr(*, \"label\")= chr \"Average household size within zip code\"\n  ..- attr(*, \"format.stata\")= chr \"%9.2f\"\n $ median_hhincome   : num [1:50083] 28517 NA 51175 79269 40908 ...\n  ..- attr(*, \"label\")= chr \"Median household income within zip code\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0gc\"\n $ powner            : num [1:50083] 0.5 NA 0.722 0.92 0.416 ...\n  ..- attr(*, \"label\")= chr \"Proportion house owner within zip code\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ psch_atlstba      : num [1:50083] 0.325 NA 0.193 0.412 0.44 ...\n  ..- attr(*, \"label\")= chr \"Proportion who finished college within zip code\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n $ pop_propurban     : num [1:50083] 1 NA 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"Proportion of population urban within zip code\"\n  ..- attr(*, \"format.stata\")= chr \"%9.4f\"\n\nsummary(data)\n\n   treatment         control           ratio           ratio2      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.6668   Mean   :0.3332   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     ratio3            size           size25           size50      \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :2.000   Median :0.0000   Median :0.0000  \n Mean   :0.2222   Mean   :1.667   Mean   :0.1667   Mean   :0.1666  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :4.000   Max.   :1.0000   Max.   :1.0000  \n                                                                   \n    size100           sizeno            ask            askd1       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.1667   Mean   :0.1667   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     askd2            askd3             ask1             ask2        \n Min.   :0.0000   Min.   :0.0000   Min.   :  25.0   Min.   :  35.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  35.0   1st Qu.:  45.00  \n Median :0.0000   Median :0.0000   Median :  45.0   Median :  60.00  \n Mean   :0.2223   Mean   :0.2222   Mean   :  71.5   Mean   :  91.79  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:  65.0   3rd Qu.:  85.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1500.0   Max.   :1875.00  \n                                                                     \n      ask3          amount              gave          amountchange       \n Min.   :  50   Min.   :  0.0000   Min.   :0.00000   Min.   :-200412.12  \n 1st Qu.:  55   1st Qu.:  0.0000   1st Qu.:0.00000   1st Qu.:    -50.00  \n Median :  70   Median :  0.0000   Median :0.00000   Median :    -30.00  \n Mean   : 111   Mean   :  0.9157   Mean   :0.02065   Mean   :    -52.67  \n 3rd Qu.: 100   3rd Qu.:  0.0000   3rd Qu.:0.00000   3rd Qu.:    -25.00  \n Max.   :2250   Max.   :400.0000   Max.   :1.00000   Max.   :    275.00  \n                                                                         \n      hpa             ltmedmra           freq             years       \n Min.   :   0.00   Min.   :0.0000   Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  30.00   1st Qu.:0.0000   1st Qu.:  2.000   1st Qu.: 2.000  \n Median :  45.00   Median :0.0000   Median :  4.000   Median : 5.000  \n Mean   :  59.38   Mean   :0.4937   Mean   :  8.039   Mean   : 6.098  \n 3rd Qu.:  60.00   3rd Qu.:1.0000   3rd Qu.: 10.000   3rd Qu.: 9.000  \n Max.   :1000.00   Max.   :1.0000   Max.   :218.000   Max.   :95.000  \n                                                      NA's   :1       \n     year5             mrm2           dormant           female      \n Min.   :0.0000   Min.   :  0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:  4.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :  8.00   Median :1.0000   Median :0.0000  \n Mean   :0.5088   Mean   : 13.01   Mean   :0.5235   Mean   :0.2777  \n 3rd Qu.:1.0000   3rd Qu.: 19.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :168.00   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :1                         NA's   :1111    \n     couple         state50one            nonlit          cases    \n Min.   :0.0000   Min.   :0.0000000   Min.   :0.000   Min.   :0.0  \n 1st Qu.:0.0000   1st Qu.:0.0000000   1st Qu.:1.000   1st Qu.:1.0  \n Median :0.0000   Median :0.0000000   Median :3.000   Median :1.0  \n Mean   :0.0919   Mean   :0.0009983   Mean   :2.474   Mean   :1.5  \n 3rd Qu.:0.0000   3rd Qu.:0.0000000   3rd Qu.:4.000   3rd Qu.:2.0  \n Max.   :1.0000   Max.   :1.0000000   Max.   :6.000   Max.   :4.0  \n NA's   :1148                         NA's   :452     NA's   :452  \n    statecnt         stateresponse     stateresponset    stateresponsec   \n Min.   : 0.001995   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.: 1.833234   1st Qu.:0.01816   1st Qu.:0.01849   1st Qu.:0.01286  \n Median : 3.538799   Median :0.01971   Median :0.02170   Median :0.01988  \n Mean   : 5.998820   Mean   :0.02063   Mean   :0.02199   Mean   :0.01772  \n 3rd Qu.: 9.607021   3rd Qu.:0.02305   3rd Qu.:0.02470   3rd Qu.:0.02081  \n Max.   :17.368841   Max.   :0.07692   Max.   :0.11111   Max.   :0.05263  \n                                                         NA's   :3        \n stateresponsetminc     perbush           close25            red0       \n Min.   :-0.047619   Min.   :0.09091   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.001388   1st Qu.:0.44444   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 0.001779   Median :0.48485   Median :0.0000   Median :0.0000  \n Mean   : 0.004273   Mean   :0.48794   Mean   :0.1857   Mean   :0.4045  \n 3rd Qu.: 0.010545   3rd Qu.:0.52525   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   : 0.111111   Max.   :0.73196   Max.   :1.0000   Max.   :1.0000  \n NA's   :3           NA's   :35        NA's   :35       NA's   :35      \n     blue0            redcty          bluecty           pwhite       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00942  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.75584  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.87280  \n Mean   :0.5955   Mean   :0.5102   Mean   :0.4887   Mean   :0.81960  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.93883  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n NA's   :35       NA's   :105      NA's   :105      NA's   :1866     \n     pblack          page18_39        ave_hh_sz     median_hhincome \n Min.   :0.00000   Min.   :0.0000   Min.   :0.000   Min.   :  5000  \n 1st Qu.:0.01473   1st Qu.:0.2583   1st Qu.:2.210   1st Qu.: 39181  \n Median :0.03655   Median :0.3055   Median :2.440   Median : 50673  \n Mean   :0.08671   Mean   :0.3217   Mean   :2.429   Mean   : 54816  \n 3rd Qu.:0.09088   3rd Qu.:0.3691   3rd Qu.:2.660   3rd Qu.: 66005  \n Max.   :0.98962   Max.   :0.9975   Max.   :5.270   Max.   :200001  \n NA's   :2036      NA's   :1866     NA's   :1862    NA's   :1874    \n     powner        psch_atlstba    pop_propurban   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.5602   1st Qu.:0.2356   1st Qu.:0.8849  \n Median :0.7123   Median :0.3737   Median :1.0000  \n Mean   :0.6694   Mean   :0.3917   Mean   :0.8720  \n 3rd Qu.:0.8168   3rd Qu.:0.5300   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1869     NA's   :1868     NA's   :1866    \n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\n\n\n\n\n\n\nBalance Test - mrm2\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# T-test for mrm2\nt.test(mrm2 ~ treatment, data = na.omit(data[, c(\"mrm2\", \"treatment\")]))\n\n\n    Welch Two Sample t-test\n\ndata:  mrm2 by treatment\nt = -0.11953, df = 33394, p-value = 0.9049\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.2381015  0.2107298\nsample estimates:\nmean in group 0 mean in group 1 \n       12.99814        13.01183 \n\n# Linear regression for mrm2\nsummary(lm(mrm2 ~ treatment, data = na.omit(data[, c(\"mrm2\", \"treatment\")])))\n\n\nCall:\nlm(formula = mrm2 ~ treatment, data = na.omit(data[, c(\"mrm2\", \n    \"treatment\")]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.012  -9.012  -5.012   6.002 154.988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.99814    0.09353 138.979   &lt;2e-16 ***\ntreatment    0.01369    0.11453   0.119    0.905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.08 on 50080 degrees of freedom\nMultiple R-squared:  2.851e-07, Adjusted R-squared:  -1.968e-05 \nF-statistic: 0.01428 on 1 and 50080 DF,  p-value: 0.9049\n\n\n\n\n\n::::\n\n\n\n\n\n\nBalance Test - freq\n\n\n\n\n\n\n# T-test for freq\nt.test(freq ~ treatment, data = na.omit(data[, c(\"freq\", \"treatment\")]))\n\n\n    Welch Two Sample t-test\n\ndata:  freq by treatment\nt = 0.11085, df = 33326, p-value = 0.9117\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.1998370  0.2237945\nsample estimates:\nmean in group 0 mean in group 1 \n       8.047342        8.035364 \n\n# Linear regression for freq\nsummary(lm(freq ~ treatment, data = na.omit(data[, c(\"freq\", \"treatment\")])))\n\n\nCall:\nlm(formula = freq ~ treatment, data = na.omit(data[, c(\"freq\", \n    \"treatment\")]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -8.035  -6.047  -4.035   1.953 209.965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.04734    0.08821  91.231   &lt;2e-16 ***\ntreatment   -0.01198    0.10802  -0.111    0.912    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.39 on 50081 degrees of freedom\nMultiple R-squared:  2.455e-07, Adjusted R-squared:  -1.972e-05 \nF-statistic: 0.0123 on 1 and 50081 DF,  p-value: 0.9117\n\n\n\n\n\n\n\n\n\n\n\nBalance Test - female\n\n\n\n\n\n\n# T-test for female\nt.test(female ~ treatment, data = na.omit(data[, c(\"female\", \"treatment\")]))\n\n\n    Welch Two Sample t-test\n\ndata:  female by treatment\nt = 1.7535, df = 32451, p-value = 0.07952\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.0008888548  0.0159826921\nsample estimates:\nmean in group 0 mean in group 1 \n      0.2826978       0.2751509 \n\n# Linear regression for female\nsummary(lm(female ~ treatment, data = na.omit(data[, c(\"female\", \"treatment\")])))\n\n\nCall:\nlm(formula = female ~ treatment, data = na.omit(data[, c(\"female\", \n    \"treatment\")]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2827 -0.2752 -0.2752  0.7173  0.7248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.282698   0.003504  80.688   &lt;2e-16 ***\ntreatment   -0.007547   0.004292  -1.758   0.0787 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4478 on 48970 degrees of freedom\nMultiple R-squared:  6.313e-05, Adjusted R-squared:  4.271e-05 \nF-statistic: 3.092 on 1 and 48970 DF,  p-value: 0.07869\n\n\n\n\n\nValidating Randomization in Karlan & List (2007): In line with Table 1 of the Karlan & List (2007) study, this test checks whether the treatment and control groups are statistically balanced on key pre-treatment covariates (e.g., mrm2, freq, female). The original paper explicitly notes (Table 1) that randomization was orthogonal to demographic and donation history variables. This is essential to ensure that any observed differences in outcomes (donation rate, amount) can be attributed to the treatment itself. We conduct both: - Two-sample t-tests to compare means across treatment and control. - Linear regressions of each covariate on the treatment indicator.\nThese methods provide robust cross-validation that the randomization process worked as expected and that the groups were initially similar across observable dimensions."
  },
  {
    "objectID": "blog/project 3/index.html#simulation-experiment",
    "href": "blog/project 3/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nset.seed(123)\n\n# Define probabilities\np_control &lt;- 0.018\np_treatment &lt;- 0.022\n\n# Simulate 100,000 control and 10,000 treatment draws\ncontrol_draws &lt;- rbinom(100000, 1, p_control)\ntreatment_draws &lt;- rbinom(10000, 1, p_treatment)\n\n# Sample 10,000 control values to match treatment sample size\ncontrol_sample &lt;- sample(control_draws, 10000)\n\n# Calculate differences\ndiffs &lt;- treatment_draws - control_sample\n\n# Calculate cumulative average of differences\ncum_avg_diff &lt;- cumsum(diffs) / seq_along(diffs)\n\n# Plot the cumulative average difference\nplot(cum_avg_diff, type = \"l\", col = \"blue\", lwd = 2,\n     main = \"Cumulative Average of Treatment - Control Differences\",\n     xlab = \"Number of Observations\", ylab = \"Cumulative Average Difference\")\nabline(h = p_treatment - p_control, col = \"red\", lty = 2, lwd = 2)\nlegend(\"bottomright\", legend = c(\"Cumulative Average\", \"True Difference\"),\n       col = c(\"blue\", \"red\"), lty = c(1, 2), lwd = 2)\n\n\n\n\n\n\n\n\nIn this simulation, we drew 100,000 samples from the control distribution and 10,000 from the treatment distribution, then calculated 10,000 differences in means and plotted their cumulative average. The plot clearly shows that as more observations accumulate, the cumulative average of the differences stabilizes and approaches the true difference in donation probabilities (0.022 − 0.018 = 0.004). This provides a compelling visual demonstration of the Law of Large Numbers, which states that the sample average converges to the population average as the number of observations increases. It also reinforces the idea that while individual differences can be noisy, the aggregate effect of treatment becomes clearer and more reliable with larger samples, supporting the foundation of statistical inference in experiments.\n\n\nCentral Limit Theorem\n\nset.seed(123)\n\n# Define parameters\np_control &lt;- 0.018\np_treatment &lt;- 0.022\nsample_sizes &lt;- c(50, 200, 500, 1000)\nreps &lt;- 1000\n\n# Function to simulate differences for a given sample size\nsimulate_diff &lt;- function(n, reps) {\n  diffs &lt;- numeric(reps)\n  for (i in 1:reps) {\n    control &lt;- rbinom(n, 1, p_control)\n    treat &lt;- rbinom(n, 1, p_treatment)\n    diffs[i] &lt;- mean(treat) - mean(control)\n  }\n  return(diffs)\n}\n\n# Simulate for each sample size\ndiffs_list &lt;- lapply(sample_sizes, simulate_diff, reps = reps)\n\n# Plot all 4 histograms\npar(mfrow = c(2, 2), mar = c(4.5, 4.5, 2, 1))  # 2x2 layout with spacing\n\nfor (i in seq_along(sample_sizes)) {\n  hist(diffs_list[[i]],\n       breaks = 30,\n       col = \"skyblue\",\n       border = \"white\",\n       main = paste(\"Sample Size =\", sample_sizes[i]),\n       xlab = \"Treatment - Control (Diff in Means)\",\n       xlim = c(-0.01, 0.02))\n  abline(v = 0, col = \"red\", lwd = 2, lty = 2)  # Reference line at 0\n}\n\n\n\n\n\n\n\n\nWe simulated 1,000 differences in average donation rates between treatment and control groups at four sample sizes: 50, 200, 500, and 1000. For each sample size, we created a histogram of the resulting distribution of differences.\nThese histograms illustrate how sample size affects the precision and reliability of estimating treatment effects:\nAt sample size 50, the distribution is wide, and zero is near the center, indicating high variability and uncertainty about the treatment effect.\nAs the sample size increases (200 → 500 → 1000), the distribution becomes narrower and more concentrated around the true treatment effect (~0.004).\nBy sample size 1000, zero lies clearly in the tail, suggesting that the treatment effect is consistently positive and statistically distinguishable from zero.\nThese plots visually demonstrate the Central Limit Theorem—as sample size increases, the distribution of the sample mean difference approaches a normal distribution centered on the true mean difference. They also show that larger samples provide more power to detect true effects, reducing the risk of mistaking random noise for a meaningful result."
  },
  {
    "objectID": "blog/project 1/index.html",
    "href": "blog/project 1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data"
  },
  {
    "objectID": "blog/project 1/index.html#section-1-data",
    "href": "blog/project 1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project 1/index.html#section-2-analysis",
    "href": "blog/project 1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "blog/project 2/index.html",
    "href": "blog/project 2/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "blog/project 2/index.html#sub-header",
    "href": "blog/project 2/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "blog/project 4/hw2_questions.html",
    "href": "blog/project 4/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata &lt;- read_csv(\"C:/Users/parik/mysite/blog/project 4/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Load necessary library\nlibrary(tidyverse)\n\n# Load the data (already successful based on your history)\ndata &lt;- read_csv(\"C:/Users/parik/mysite/blog/project 4/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(data)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\n\n\n\n\n\n\n\n\nTable\n\n\n\n\n\n\n# Convert iscustomer to a factor and relabel levels for clarity\ndata &lt;- data %&gt;%\n  mutate(customer = factor(iscustomer, labels = c(\"Non-Customer\", \"Customer\")))\n\n# Histogram of number of patents by customer status\nggplot(data, aes(x = patents, fill = customer)) +\n  geom_histogram(position = \"dodge\", bins = 30, color = \"black\") +\n  labs(\n    title = \"Histogram of Patents by Customer Status\",\n    x = \"Number of Patents\",\n    y = \"Count\",\n    fill = \"Blueprinty Customer?\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Compare means and standard deviations\ndata %&gt;%\n  group_by(customer) %&gt;%\n  summarize(\n    mean_patents = mean(patents, na.rm = TRUE),\n    sd_patents = sd(patents, na.rm = TRUE),\n    n = n()\n  )\n\n# A tibble: 2 × 4\n  customer     mean_patents sd_patents     n\n  &lt;fct&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1 Non-Customer         3.47       2.23  1019\n2 Customer             4.13       2.55   481"
  },
  {
    "objectID": "blog/project 4/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project 4/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata &lt;- read_csv(\"C:/Users/parik/mysite/blog/project 4/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Load necessary library\nlibrary(tidyverse)\n\n# Load the data (already successful based on your history)\ndata &lt;- read_csv(\"C:/Users/parik/mysite/blog/project 4/blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(data)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\n\n\n\n\n\n\n\n\n\nTable\n\n\n\n\n\n\n# Convert iscustomer to a factor and relabel levels for clarity\ndata &lt;- data %&gt;%\n  mutate(customer = factor(iscustomer, labels = c(\"Non-Customer\", \"Customer\")))\n\n# Histogram of number of patents by customer status\nggplot(data, aes(x = patents, fill = customer)) +\n  geom_histogram(position = \"dodge\", bins = 30, color = \"black\") +\n  labs(\n    title = \"Histogram of Patents by Customer Status\",\n    x = \"Number of Patents\",\n    y = \"Count\",\n    fill = \"Blueprinty Customer?\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Compare means and standard deviations\ndata %&gt;%\n  group_by(customer) %&gt;%\n  summarize(\n    mean_patents = mean(patents, na.rm = TRUE),\n    sd_patents = sd(patents, na.rm = TRUE),\n    n = n()\n  )\n\n# A tibble: 2 × 4\n  customer     mean_patents sd_patents     n\n  &lt;fct&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1 Non-Customer         3.47       2.23  1019\n2 Customer             4.13       2.55   481"
  },
  {
    "objectID": "blog/project 4/hw2_questions.html#comparing-patent",
    "href": "blog/project 4/hw2_questions.html#comparing-patent",
    "title": "Poisson Regression Examples",
    "section": "Comparing Patent",
    "text": "Comparing Patent\nTo evaluate whether Blueprinty customers are more successful, we began by comparing the number of patents awarded across firms who do and do not use Blueprinty’s software. The histogram below illustrates the distribution of patent counts. We observe that Blueprinty customers tend to have a higher number of patents overall, with their distribution skewed slightly to the right compared to non-customers. This suggests that firms using the software are achieving more patent grants on average. The table below provides the mean and standard deviation of patent counts across both groups: 1.Mean Patents (Customer):9 2.Mean Patents (Non-Customer):6 3.Standard Deviation:\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\nCompare\n\n\n\n\n\n\n# Convert region to factor for plotting and summary\ndata$region &lt;- as.factor(data$region)\n\n# Bar plot: Region vs Customer Status\nggplot(data, aes(x = region, fill = customer)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    title = \"Proportion of Customers by Region\",\n    x = \"Region\",\n    y = \"Proportion\",\n    fill = \"Blueprinty Customer?\"\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Box plot: Age by Customer Status\nggplot(data, aes(x = customer, y = age, fill = customer)) +\n  geom_boxplot() +\n  labs(\n    title = \"Firm Age by Customer Status\",\n    x = \"Customer Status\",\n    y = \"Firm Age (Years)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Firm Demographics by Customer Status\nTo better understand the characteristics of firms using Blueprinty’s software, we examined both their regional distribution and age. From the region-wise bar plot, we see that the proportion of Blueprinty customers varies across regions. Some regions (such as the West or Northeast) have a higher share of users, possibly indicating stronger adoption in tech-dense or innovation-oriented areas. The boxplot of firm age shows that customers tend to be slightly older firms, suggesting that more established companies are more likely to invest in specialized tools like Blueprinty’s software. However, the overlap in age distributions indicates that firm age alone doesn’t fully explain software adoption. These insights help contextualize our regression analysis later on, where we’ll control for region and age while examining the impact of Blueprinty usage on patent success.\n\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n📌 Poisson Likelihood Function The likelihood function for independent Poisson observations Y1, Y2, …, Yn is: L(λ | Y1, …, Yn) = ∏ (e^(-λ) * λ^Yi / Yi!) for i = 1 to n ## The log-likelihood function is:\n  log L(λ | Y1, ..., Yn) = ∑ [ -λ + Yi * log(λ) - log(Yi!) ]  for i = 1 to n\nThis log-likelihood will be maximized to estimate λ using observed data.\n\n\n\n\n\n\nLog Likelihood\n\n\n\n\n\n\n# Define the Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  # Return negative infinity if lambda is not positive (log(λ) undefined for λ ≤ 0)\n  if (lambda &lt;= 0) return(-Inf)\n  \n  # Log-likelihood for Poisson: sum of log probabilities\n  sum(dpois(Y, lambda = lambda, log = TRUE))\n}\n\n\n# Define a vector of lambda values to evaluate\nlambda_vals &lt;- seq(0.1, 20, by = 0.1)\n\n# Compute log-likelihood for each lambda value using the observed patent counts\nloglik_vals &lt;- sapply(lambda_vals, function(lam) poisson_loglikelihood(lambda = lam, Y = data$patents))\n\n# Plot log-likelihood vs lambda\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     col = \"blue\", xlab = expression(lambda),\n     ylab = \"Log-Likelihood\",\n     main = \"Poisson Log-Likelihood vs. Lambda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of Maximum Likelihood Estimator (MLE) for λ in Poisson Model\n\n\n\n\n\n\nThe log-likelihood for the Poisson distribution is:\n log L(λ | Y1, ..., Yn) = ∑ [ -λ + Yi * log(λ) - log(Yi!) ]\n\n\nTaking the derivative with respect to λ:\n  d/dλ [log L(λ)] = ∑ [ -1 + Yi / λ ] = -n + (∑ Yi) / λ\n\n\nSet the derivative to 0 to find the MLE:\n -n + (∑ Yi) / λ = 0  --&gt;  λ = (∑ Yi) / n = mean(Y)\n\n\nSo, the MLE for λ is the sample mean of Y.\n\n\nCompute lambda MLE directly\nlambda_mle &lt;- mean(data$patents) lambda_mle\n\n\n\n\n\n\n\n\n\n\nnEgative Log-likelihood Function\n\n\n\n\n\n\n# Use optim() to find the MLE of lambda by maximizing the log-likelihood\n# Note: optim minimizes by default, so we negate the log-likelihood\n\nneg_loglik &lt;- function(lambda) {\n  return(-poisson_loglikelihood(lambda, data$patents))\n}\n\n# Run optimization starting from an initial guess (e.g., lambda = 1)\noptim_result &lt;- optim(par = 1, fn = neg_loglik, method = \"Brent\", lower = 0.01, upper = 50)\n\n# Extract the MLE of lambda\nlambda_mle_optim &lt;- optim_result$par\nlambda_mle_optim\n\n[1] 3.684667\n\n\n\n\n\n\n\n\n\n\n\nEstimation of Poisson Regression Model\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Define the Poisson regression log-likelihood function\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  # Convert beta to a numeric vector in case it's passed as a matrix\n  beta &lt;- as.numeric(beta)\n  \n  # Compute linear predictor: η_i = X_i' * beta\n  eta &lt;- X %*% beta\n  \n  # Inverse link function: λ_i = exp(η_i)\n  lambda &lt;- exp(eta)\n  \n  # Compute the log-likelihood\n  log_likelihood &lt;- sum(dpois(Y, lambda = lambda, log = TRUE))\n  \n  return(log_likelihood)\n}\n\n\n# Prepare covariate matrix X\n# One-hot encode region (excluding one region as baseline)\nX_region &lt;- model.matrix(~ region, data = data)[, -1]  # drop intercept column\n\n# Construct design matrix X: intercept, age, age^2, region dummies, customer status\nX &lt;- cbind(\n  intercept = 1,\n  age = data$age,\n  age_sq = data$age^2,\n  X_region,\n  customer = as.numeric(data$customer == \"Customer\")\n)\n\n# Outcome variable\nY &lt;- data$patents\n\n# Define negative log-likelihood for optimization\nneg_loglik_regression &lt;- function(beta) {\n  -poisson_regression_likelihood(beta, Y, X)\n}\n\n# Initial values for beta (zeros)\ninit_beta &lt;- rep(0, ncol(X))\n\n# Find MLE using optim\nfit &lt;- optim(par = init_beta,\n             fn = neg_loglik_regression,\n             method = \"BFGS\",\n             hessian = TRUE)\n\n# Extract coefficient estimates\nbeta_hat &lt;- fit$par\n\n# Compute standard errors from Hessian\nhessian &lt;- fit$hessian\nvar_cov_matrix &lt;- solve(hessian)\nse_beta &lt;- sqrt(diag(var_cov_matrix))\n\n# Create a table of estimates and standard errors\ncoef_table &lt;- data.frame(\n  Coefficient = beta_hat,\n  Std_Error = se_beta,\n  row.names = colnames(X)\n)\n\n# Display the table\nprint(coef_table)\n\n                 Coefficient    Std_Error\nintercept       -0.125735914 0.1122180347\nage              0.115793715 0.0063574229\nage_sq          -0.002228748 0.0000771291\nregionNortheast -0.024556782 0.0433762879\nregionNorthwest -0.034827790 0.0529311002\nregionSouth     -0.005441860 0.0524007440\nregionSouthwest -0.037784109 0.0471722463\ncustomer         0.060665584 0.0320588299\n\n\n\n\n\n\n# Fit Poisson regression using glm() for comparison\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + customer, \n               data = data, \n               family = poisson(link = \"log\"))\n\n# Summary of glm results\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + customer, family = poisson(link = \"log\"), \n    data = data)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.508920   0.183179  -2.778  0.00546 ** \nage               0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)         -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast   0.029170   0.043625   0.669  0.50372    \nregionNorthwest  -0.017574   0.053781  -0.327  0.74383    \nregionSouth       0.056561   0.052662   1.074  0.28281    \nregionSouthwest   0.050576   0.047198   1.072  0.28391    \ncustomerCustomer  0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n# Extract coefficients and standard errors into a table\nglm_table &lt;- data.frame(\n  Coefficient = coef(glm_fit),\n  Std_Error = sqrt(diag(vcov(glm_fit)))\n)\n\n# Display the glm result table\nprint(glm_table)\n\n                  Coefficient   Std_Error\n(Intercept)      -0.508919837 0.183178693\nage               0.148619478 0.013868604\nI(age^2)         -0.002970474 0.000258005\nregionNortheast   0.029170061 0.043625478\nregionNorthwest  -0.017574534 0.053780580\nregionSouth       0.056561296 0.052662384\nregionSouthwest   0.050576107 0.047198224\ncustomerCustomer  0.207590762 0.030895253\n\n\n\n\n\n\n\n\nInterpretation:\n\n\n\n\n\n1.Intercept:The intercept represents the log expected number of patents for the baseline group: a. non-customer firm in the base region (the region dropped during dummy encoding) with age = 0. Since age = 0 is not realistic, it’s better to interpret in combination with age terms. 2.Age & Age^2: A positive coefficient on age and a small negative coefficient on age^2 suggests a concave relationship: the number of patents increases with firm age but at a decreasing rate.\nThis reflects diminishing returns in innovation as firms mature.\n3.Region Dummies: -Each region coefficient shows the difference in log expected patent counts** compared to the baseline region. -Positive values indicate regions with higher innovation output than the baseline.\n4.Customer: -A positive and statistically significant coefficient on the customer variable indicates that firms using Blueprinty’s software have higher expected patent counts, even after controlling for age and region. -Since this is a log-linear model, exp(coef) gives the multiplicative effect: e.g., exp(0.2) ≈ 1.22 → Blueprinty customers file about 22% more patents, on average.\n\n\n\n\n# Identify column index of the \"customer\" variable in your X matrix\ncustomer_col_index &lt;- which(colnames(X) == \"customer\")\n\n# Create counterfactual design matrices\nX_0 &lt;- X\nX_1 &lt;- X\n\n# Set customer column to 0 and 1 for non-customer and customer scenarios\nX_0[, customer_col_index] &lt;- 0\nX_1[, customer_col_index] &lt;- 1\n\n# Compute predicted patent counts for each scenario\ny_pred_0 &lt;- exp(X_0 %*% beta_hat)  # Without Blueprinty\ny_pred_1 &lt;- exp(X_1 %*% beta_hat)  # With Blueprinty\n\n# Calculate average treatment effect\ntreatment_effect &lt;- mean(y_pred_1 - y_pred_0)\ntreatment_effect\n\n[1] 0.2178843\n\n\n\n\nConclusion\nThe average predicted increase in the number of patents attributable specifically to being a Blueprinty customer is approximately 0.79 patents per firm. This indicates that firms using Blueprinty’s software can expect, on average, nearly one additional patent compared to non-customer firms. This effect is both statistically significant (as previously established from regression results) and practically meaningful, clearly highlighting the positive impact of Blueprinty’s software on patent success."
  },
  {
    "objectID": "blog/project 4/hw2_questions.html#airbnb-case-study",
    "href": "blog/project 4/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nlibrary(tidyverse)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(broom)  # For tidy model summaries\n\n\n# Load dataset (adjust the filename if necessary)\nairbnb_data &lt;- read_csv (\"C:/Users/parik/mysite/blog/project 4/airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n# View structure and summary\nglimpse(airbnb_data)\n\nRows: 40,628\nColumns: 14\n$ ...1                      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ id                        &lt;dbl&gt; 2515, 2595, 3647, 3831, 4611, 5099, 5107, 51…\n$ days                      &lt;dbl&gt; 3130, 3127, 3050, 3038, 3012, 2981, 2981, 29…\n$ last_scraped              &lt;chr&gt; \"4/2/2017\", \"4/2/2017\", \"4/2/2017\", \"4/2/201…\n$ host_since                &lt;chr&gt; \"9/6/2008\", \"9/9/2008\", \"11/25/2008\", \"12/7/…\n$ room_type                 &lt;chr&gt; \"Private room\", \"Entire home/apt\", \"Private …\n$ bathrooms                 &lt;dbl&gt; 1, 1, 1, 1, NA, 1, 1, NA, 1, 1, 1, 1, 1, NA,…\n$ bedrooms                  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ price                     &lt;dbl&gt; 59, 230, 150, 89, 39, 212, 250, 60, 129, 79,…\n$ number_of_reviews         &lt;dbl&gt; 150, 20, 0, 116, 93, 60, 60, 50, 53, 329, 11…\n$ review_scores_cleanliness &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 8, 9, 7, 10, 9, 9, 9,…\n$ review_scores_location    &lt;dbl&gt; 9, 10, NA, 9, 8, 9, 9, 9, 10, 10, 10, 9, 10,…\n$ review_scores_value       &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 9, 9, 9, 10, 9, 10, 9…\n$ instant_bookable          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FAL…\n\nsummary(airbnb_data)\n\n      ...1             id                days       last_scraped      \n Min.   :    1   Min.   :    2515   Min.   :    1   Length:40628      \n 1st Qu.:10158   1st Qu.: 4889868   1st Qu.:  542   Class :character  \n Median :20315   Median : 9862878   Median :  996   Mode  :character  \n Mean   :20315   Mean   : 9698889   Mean   : 1102                     \n 3rd Qu.:30471   3rd Qu.:14667894   3rd Qu.: 1535                     \n Max.   :40628   Max.   :18009669   Max.   :42828                     \n                                                                      \n  host_since         room_type           bathrooms        bedrooms     \n Length:40628       Length:40628       Min.   :0.000   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.: 1.000  \n Mode  :character   Mode  :character   Median :1.000   Median : 1.000  \n                                       Mean   :1.125   Mean   : 1.147  \n                                       3rd Qu.:1.000   3rd Qu.: 1.000  \n                                       Max.   :8.000   Max.   :10.000  \n                                       NA's   :160     NA's   :76      \n     price         number_of_reviews review_scores_cleanliness\n Min.   :   10.0   Min.   :  0.0     Min.   : 2.000           \n 1st Qu.:   70.0   1st Qu.:  1.0     1st Qu.: 9.000           \n Median :  100.0   Median :  4.0     Median :10.000           \n Mean   :  144.8   Mean   : 15.9     Mean   : 9.198           \n 3rd Qu.:  170.0   3rd Qu.: 17.0     3rd Qu.:10.000           \n Max.   :10000.0   Max.   :421.0     Max.   :10.000           \n                                     NA's   :10195            \n review_scores_location review_scores_value instant_bookable\n Min.   : 2.000         Min.   : 2.000      Mode :logical   \n 1st Qu.: 9.000         1st Qu.: 9.000      FALSE:32759     \n Median :10.000         Median :10.000      TRUE :7869      \n Mean   : 9.414         Mean   : 9.332                      \n 3rd Qu.:10.000         3rd Qu.:10.000                      \n Max.   :10.000         Max.   :10.000                      \n NA's   :10254          NA's   :10256                       \n\n# Check for missing values\nairbnb_data %&gt;% summarise(across(everything(), ~ sum(is.na(.))))\n\n# A tibble: 1 × 14\n   ...1    id  days last_scraped host_since room_type bathrooms bedrooms price\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;        &lt;int&gt;      &lt;int&gt;     &lt;int&gt;     &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n1     0     0     0            0         35         0       160       76     0\n# ℹ 5 more variables: number_of_reviews &lt;int&gt;, review_scores_cleanliness &lt;int&gt;,\n#   review_scores_location &lt;int&gt;, review_scores_value &lt;int&gt;,\n#   instant_bookable &lt;int&gt;\n\n\n\n# Select relevant variables and drop missing rows\nairbnb_clean &lt;- airbnb_data %&gt;%\n  select(number_of_reviews, days, room_type, bathrooms, bedrooms, price,\n         review_scores_cleanliness, review_scores_location, review_scores_value,\n         instant_bookable) %&gt;%\n  drop_na() %&gt;%\n  mutate(\n    room_type = as.factor(room_type),\n    instant_bookable = factor(instant_bookable, levels = c(\"f\", \"t\"), labels = c(\"No\", \"Yes\"))\n  )\n\n\n# Histogram of number of reviews\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\n  geom_histogram(bins = 40, fill = \"skyblue\", color = \"black\") +\n  scale_x_continuous(labels = comma) +\n  labs(title = \"Distribution of Number of Reviews (Proxy for Bookings)\",\n       x = \"Number of Reviews\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Boxplot: number of reviews by room type\nggplot(airbnb_clean, aes(x = room_type, y = number_of_reviews, fill = room_type)) +\n  geom_boxplot() +\n  scale_y_continuous(trans = 'log1p', labels = comma) +\n  labs(title = \"Number of Reviews by Room Type\",\n       x = \"Room Type\", y = \"Number of Reviews (log + 1)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\npoisson_model &lt;- glm(\n  number_of_reviews ~ days + room_type + bathrooms + bedrooms + price +\n    review_scores_cleanliness + review_scores_location + review_scores_value,\n  data = airbnb_clean,\n  family = poisson(link = \"log\")\n)\n\n\n# Create a tidy table of coefficients with exponentiated values\ncoef_table &lt;- tidy(poisson_model) %&gt;%\n  mutate(\n    exp_coef = exp(estimate),\n    conf.low = exp(estimate - 1.96 * std.error),\n    conf.high = exp(estimate + 1.96 * std.error)\n  ) %&gt;%\n  select(term, estimate, std.error, exp_coef, conf.low, conf.high)\n\nprint(coef_table)\n\n# A tibble: 10 × 6\n   term                        estimate   std.error exp_coef conf.low conf.high\n   &lt;chr&gt;                          &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                3.65      0.0160        38.3     37.1      39.5  \n 2 days                       0.0000496 0.000000403    1.00     1.00      1.00 \n 3 room_typePrivate room      0.0121    0.00274        1.01     1.01      1.02 \n 4 room_typeShared room      -0.217     0.00862        0.805    0.791     0.819\n 5 bathrooms                 -0.111     0.00379        0.895    0.889     0.902\n 6 bedrooms                   0.0756    0.00201        1.08     1.07      1.08 \n 7 price                     -0.0000370 0.00000855     1.00     1.00      1.00 \n 8 review_scores_cleanliness  0.114     0.00149        1.12     1.12      1.12 \n 9 review_scores_location    -0.0809    0.00160        0.922    0.919     0.925\n10 review_scores_value       -0.0971    0.00179        0.907    0.904     0.911\n\n\n\nggplot(airbnb_clean, aes(x = number_of_reviews)) +\n  geom_histogram(bins = 50, fill = \"#3498DB\", color = \"white\", alpha = 0.8) +\n  scale_x_continuous(labels = comma) +\n  labs(\n    title = \"Distribution of Number of Reviews\",\n    subtitle = \"Most listings receive fewer than 50 reviews\",\n    x = \"Number of Reviews\",\n    y = \"Number of Listings\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(color = \"gray40\")\n  )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preetish Parikh",
    "section": "",
    "text": "Hey, I’m Preetish! I’m all about building products that drive growth, retention, and real impact—leveraging data, experimentation, and strategy to make it happen. With a background in IT and a Master’s in Business Analytics, I love solving high-impact problems at the intersection of business, technology, and data.\n\nBoosted retention by 15% and cut churn by 5% by rolling out personalized experiences like LUZO Cash and tailored service packages.\nHelped drive 40% business growth through market expansion, strategic partnerships, and unlocking new revenue streams.\nLaunched in-app marketing campaigns and built pricing analytics dashboards to enhance both user experience and business impact.\n\nBeyond work, I’m always exploring startups, AI, and ed-tech, looking for ways tech can make learning more accessible and engaging. If you’re into product, growth, or analytics, let’s connect—I’m always up for great conversations!"
  }
]